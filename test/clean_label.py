# -*- coding: utf-8 -*-
"""Gurbaaz-Clean-Label.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w3doNAXY55IPBpJm8qI-yyjdZsslGKAq

**Cloning the repo**
"""

# !git clone https://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@github.com/gurbaaz27/cs776-course-project/

# # Commented out IPython magic to ensure Python compatibility.
# # %cp -r cs776-course-project/src .

# # Commented out IPython magic to ensure Python compatibility.
# !pip3 install adversarial-robustness-toolbox

from __future__ import absolute_import, division, print_function, unicode_literals

import os, sys
from os.path import abspath

module_path = os.path.abspath(os.path.join('..'))
if module_path not in sys.path:
    sys.path.append(module_path)

import warnings
warnings.filterwarnings('ignore')
import keras.backend as k
from keras.models import Sequential
from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Activation, Dropout
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

import tensorflow as tf
tf.compat.v1.disable_eager_execution()
tf.get_logger().setLevel('ERROR')

from src.classifiers.kerasclassifier import KerasClassifier
from src.attacks.clean_label_backdoor import (
    PoisoningAttackBackdoor,
    PoisoningAttackCleanLabelBackdoor,
)
from src.attacks.utils import add_pattern_bd
from src.utils import load_mnist, preprocess, to_categorical
from src.defences.adversarial_trainer_madry_pgd import (
    AdversarialTrainerMadryPGD,
)



(x_raw, y_raw), (x_raw_test, y_raw_test), min_, max_ = load_mnist(raw=True)

# Random Selection:
n_train = np.shape(x_raw)[0]
num_selection = 10000
random_selection_indices = np.random.choice(n_train, num_selection)
x_raw = x_raw[random_selection_indices]
y_raw = y_raw[random_selection_indices]

# Poison training data
percent_poison = .33
x_train, y_train = preprocess(x_raw, y_raw)
x_train = np.expand_dims(x_train, axis=3)

x_test, y_test = preprocess(x_raw_test, y_raw_test)
x_test = np.expand_dims(x_test, axis=3)
  
# Shuffle training data
n_train = np.shape(y_train)[0]
shuffled_indices = np.arange(n_train)
np.random.shuffle(shuffled_indices)
x_train = x_train[shuffled_indices]
y_train = y_train[shuffled_indices]

# Create Keras convolutional neural network - basic architecture from Keras examples
# Source here: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py
def create_model():    
    model = Sequential()
    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=x_train.shape[1:]))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(10, activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

backdoor = PoisoningAttackBackdoor(add_pattern_bd)
example_target = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])
pdata, plabels = backdoor.poison(x_test, y=example_target)

plt.imshow(pdata[0].squeeze())

# Poison some percentage of all non-nines to nines
targets = to_categorical([9], 10)[0] 

proxy = AdversarialTrainerMadryPGD(KerasClassifier(create_model()), nb_epochs=10, eps=0.15, eps_step=0.001)
proxy.fit(x_train, y_train)

backdoor = PoisoningAttackBackdoor(add_pattern_bd)
attack = PoisoningAttackCleanLabelBackdoor(backdoor=backdoor, trained_classifier=proxy.get_classifier(),
                                           target=targets, pp_poison=percent_poison, norm=2, eps=5,
                                           eps_step=0.1, max_iter=200)
pdata, plabels = attack.poison(x_train, y_train)

poisoned = pdata[np.all(plabels == targets, axis=1)]
poisoned_labels = plabels[np.all(plabels == targets, axis=1)]
print(len(poisoned))
for i in range(len(poisoned)):
    if poisoned[i][0][0] != 0:
        plt.imshow(poisoned[i].squeeze())
        plt.show()
        print(f"Index: {i} Label: {np.argmax(poisoned_labels[i])}")
        break

print(len(pdata))

print(len(plabels))

model = KerasClassifier(create_model())

model.fit(pdata, plabels, nb_epochs=10)

not_target = np.logical_not(np.all(y_test == targets, axis=1))
px_test, py_test = backdoor.poison(x_test[not_target], y_test[not_target])

poison_preds = np.argmax(model.predict(px_test), axis=1)
clean_correct = np.sum(poison_preds == np.argmax(y_test[not_target], axis=1))
clean_total = y_test.shape[0]

clean_acc = clean_correct / clean_total
print("\nPoison test set accuracy (model): %.2f%%" % (clean_acc * 100))

c = 0 # index to display
plt.imshow(px_test[c].squeeze())
plt.show()
clean_label = c
print("Prediction: " + str(poison_preds[c]))

# Commented out IPython magic to ensure Python compatibility.
warnings.filterwarnings('ignore')

# from art import config
# from art.utils import load_dataset, get_file
# from art.estimators.classification import KerasClassifier
# from art.attacks.evasion import FastGradientMethod
# from art.attacks.evasion import BasicIterativeMethod
from src.defences.adversarial_trainer import AdversarialTrainer

import numpy as np

# %matplotlib inline
import matplotlib.pyplot as plt

from src.attacks.projected_gradient_descent import ProjectedGradientDescent

robust_classifier = KerasClassifier(create_model())
attacks = ProjectedGradientDescent(robust_classifier, eps=0.3, eps_step=0.01, max_iter=40)
trainer = AdversarialTrainer(robust_classifier, attacks, ratio=1.0)
trainer.fit(pdata, plabels, nb_epochs=8, batch_size=250)

not_target = np.logical_not(np.all(y_test == targets, axis=1))
px_test, py_test = backdoor.poison(x_test[not_target], y_test[not_target])
poison_preds = np.argmax(robust_classifier.predict(px_test), axis=1)
clean_correct = np.sum(poison_preds == np.argmax(y_test[not_target], axis=1))
clean_total = py_test.shape[0]
print(y_test.shape[0])
clean_acc = clean_correct / clean_total
print("\nPoison test set accuracy (model): %.2f%%" % (clean_acc * 100))

c = 0 # index to display
plt.imshow(px_test[c].squeeze())
plt.show()
clean_label = c
print("Prediction: " + str(poison_preds[c]))

plt.imshow(pdata[0].squeeze())

plt.imshow(pdata[2].squeeze())

poisoned = pdata[np.all(plabels == targets, axis=1)]

for i in range(len(pdata)):
  if plabels[i][9]==1:
    print(i)

